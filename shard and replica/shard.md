Шардирование

Шардирование — это техника логического разделения данных на независимые сегменты, называемые шардами, каждый из которых может храниться и обрабатываться отдельным сервером. Благодаря этому можно значительно повысить пропускную способность системы и устранить узкие места в производительности. Важно, что каждый элемент данных принадлежит только одному шару, что упрощает маршрутизацию запросов и минимизирует избыточность.
Типы шардирования

Существует два основных типа шардирования:

    Горизонтальный — разделение данных по строкам: каждая таблица дублируется на всех узлах, но содержит разные записи.
    Вертикальный — разделение по столбцам: каждый узел хранит только определённые поля таблиц или подмножества таблиц, соответствующие различным подсистемам.

[Схема 1: Горизонтальное vs. вертикальное шардирование]

Методов определения, к какому шарду должны попасть данные, существует множество, каждый из которых подходит под разные сценарии использования:

    Хеширование ключа (hash-based) — применяется хеш-функция к уникальному ключу записи. Полученное значение определяет шард, в который отправятся данные. Этот подход обеспечивает равномерное распределение, но затрудняет выполнение диапазонных запросов.
    Диапазон значений (range-based) — данные распределяются по диапазонам значений, например: ID 1–1000 в один шард, 1001–2000 в другой. Удобен для аналитических и выборочных операций, но подвержен риску "горячих точек" при неравномерном росте данных.
    Географическое шардирование (geo-based) — данные делятся по региональному признаку, чтобы минимизировать сетевые задержки. Часто используется в глобальных системах для локализации трафика.
    Функциональное (или доменное) шардирование — распределение по логике приложения: одни таблицы (например, пользователи) идут в один кластер, другие (например, заказы или логи) — в другой. Часто применяется в микросервисных архитектурах.
    Динамический или автоматический шардинг — распределение и перераспределение данных осуществляется автоматически в зависимости от текущей нагрузки, объёма или плотности данных. Такие механизмы используются, например, в MongoDB, CockroachDB или Vitess.

[Схема 2: Примеры методов шардирования]
Добавление нового шарда

С увеличением объёма данных и ростом нагрузки на систему может возникнуть необходимость в добавлении новых шардов для поддержания масштабируемости. Это достаточно сложный процесс, который требует точной координации и внимательного подхода, чтобы минимизировать влияние на производительность и доступность системы.
Основные подходы добавления нового шарда

Процесс добавления нового шарда зависит от используемой архитектуры и инструментов, однако существует несколько общих стратегий, которые могут быть применены в разных случаях.
Ручное решардирование

Решардирование вручную — это базовый подход, при котором данные переносятся с существующих шардов на новый вручную, а затем обновляется конфигурация маршрутизации запросов. Это довольно трудоёмкий процесс, требующий точности, поскольку ошибки могут привести к несогласованности данных или к нарушению работы системы.

Решардирование вручную обычно включает несколько этапов:

    Создание нового шарда с соответствующей конфигурацией.
    Перераспределение данных, например, с помощью скриптов или специализированных утилит.
    Обновление маршрутизации запросов, чтобы новые данные могли быть направлены на свежий шард.
    Обеспечение целостности данных в процессе переноса.

Этот метод подходит для систем, где перерывы в обслуживании или небольшие простои могут быть допустимы, или для простых систем с относительно небольшим количеством шардов. Преимущества:

    Полный контроль над процессом.
    Гибкость в решении уникальных задач.

[Схема 3: Процесс ручного решардирования]
Решардирование с использованием Middleware

Множество современных распределённых СУБД применяют middleware решения, такие как Vitess, Citus или Yugabyte, которые автоматизируют процесс ребалансировки данных и обновления маршрутов запросов без необходимости в даунтайме. Эти инструменты абстрагируют сложность решардирования и обеспечивают непрерывную работу системы в процессе расширения.

Vitess (к примеру) управляет шардами и отвечает за автомат* автоматическое перераспределение данных, когда добавляются новые шарды. Это позволяет избежать простоя системы и минимизировать риски, связанные с человеческим фактором. Преимущества:

    Автоматизация процесса решардирования.
    Минимизация простоя и рисков ошибок.
    Высокая степень интеграции с современными распределёнными СУБД.

[Схема 4: Решардирование с использованием middleware]
Прокси или маршрутизаторы

Прокси-сервера или маршрутизаторы — это подход, при котором добавление новых шардов происходит без изменений на стороне клиента. В этом случае на промежуточном уровне (например, с использованием инструмента типа ProxySQL или Vitess Proxy) работает маршрутизатор, который управляет распределением запросов и определяет, какой шард обслуживает каждый запрос.

Когда добавляется новый шард, маршрутизатор автоматически перенаправляет запросы на правильные узлы, что позволяет масштабировать систему без необходимости изменять клиентскую логику. Преимущества:

    Отсутствие изменений в логике приложения.
    Простота интеграции новых шардов.
    Снижение операционных рисков, связанных с изменениями на клиентской стороне.

[Схема 5: Работа прокси-роутера]
Online Resharding и Dual Writes

Чтобы минимизировать даунтайм и риски потери данных, при добавлении нового шарда часто используется online resharding. Этот метод включает в себя использование dual writes — временной записи данных как в старые, так и в новые шарды. Во время такого процесса данные пишутся одновременно в оба шарда (старый и новый), а затем проводится фоновый перенос данных на новый шард.

Процесс может включать следующие этапы:

    Включение временной записи в оба шардовых узла (старый и новый).
    Постепенный перенос данных на новый шард в фоновом режиме.
    По завершению переноса данных — остановка записи на старый шард.

Этот метод помогает свести к минимуму время простоя и гарантировать, что все записи данных будут успешными, даже если они происходят во время переноса. Преимущества:

    Минимизация времени простоя.
    Обеспечение целостности данных в процессе переноса.

[Схема 6: Процесс online resharding]
Балансировка нагрузки

Шардирование должно обеспечивать равномерную нагрузку на все узлы. Без должной балансировки даже при наличии множества шардов часть из них может перегружаться, в то время как другие остаются незадействованными.

    При range-шардировании часто возникают горячие шарды, особенно если новые записи всегда попадают в один и тот же диапазон.
    Hash-шардирование даёт более равномерное распределение, но при добавлении новых шардов необходимо перераспределение ключей.

Для минимизации горячих точек можно использовать согласованное хэширование (consistent hashing), которое минимизирует перемещение данных при добавлении или удалении узлов. Алгоритм назначает данные ближайшему узлу на кольце хэшей, что упрощает масштабирование.

Практический вопрос: Как обнаружить и устранить горячие точки?

    Ответ: Мониторьте метрики нагрузки на шарды (например, queries_per_shard в Citus или cpu_usage_per_shard в Vitess) с помощью Prometheus и Grafana. Если один шард перегружен, перераспределите данные с помощью rebalance_table_shards (Citus) или Reshard (Vitess). Для предотвращения используйте хэширование по ключу с высокой кардинальностью, например, user_id вместо region.

[Схема 7: Горячие точки и ребалансировка]
Affinity и Anti-Affinity

По мере роста объёма данных и распределения нагрузки по нескольким шардовым узлам перед разработчиками встаёт не только задача эффективного масштабирования, но и вопрос логического размещения данных. Одним из ключевых принципов, влияющих на производительность и устойчивость системы, становится выбор между affinity и anti-affinity стратегиями при шардинге.
Affinity: стремление к связности

Affinity (от англ. "связанность") — это архитектурный принцип, согласно которому логически связанные сущности (например, пользователь и его заказы) хранятся в одном и том же шарде. Такой подход облегчает выполнение транзакций, ускоряет JOIN-запросы и снижает частоту кросс-шардового взаимодействия.

Представим себе систему электронной коммерции: пользователь, его корзина, заказы и платежи — все эти данные постоянно взаимодействуют друг с другом. Если они размещены на одном узле, система может обрабатывать типичные запросы (например, «показать историю заказов пользователя») без необходимости обращаться к другим шардовым нодам. Это особенно критично для OLTP-нагрузок, где важны транзакционность и задержки. Преимущества affinity-шардинга:

    Минимизация сетевого взаимодействия. Все данные находятся рядом — запросы не пересекают границы шардов.
    Ускорение транзакций. В большинстве СУБД транзакции внутри одного узла значительно быстрее и проще, чем мультишардовые.
    Простота обеспечения согласованности. Принципы ACID легко соблюдаются, поскольку нет необходимости координировать изменения между несколькими шардами.

[Схема 8: Affinity-шардирование]

Пример: В Shopify данные магазина и его заказов хранятся на одном шарде, что ускоряет запросы типа SELECT * FROM orders WHERE shop_id = ?.
Anti-Affinity: диверсификация и устойчивость

В противоположность подходу "всё в одном месте", anti-affinity предполагает преднамеренное разнесение логически связанных сущностей по разным шардам. На первый взгляд это усложняет архитектуру, но на практике часто оказывается необходимым для повышения отказоустойчивости и распределения нагрузки.

Такой подход полезен, например, в системах, где одна группа данных может полностью "захватить" узел. Допустим, крупный клиент в CRM-системе начинает активно генерировать активность (например, тысячами создаёт записи). Если все его данные хранятся на одном узле — это быстро приводит к перегрузке. Разнесение данных клиента по нескольким шардам помогает сбалансировать нагрузку. Преимущества anti-affinity-шардинга:

    Изоляция сбоев. Сбой одного шарда не приводит к полной недоступности данных определённого пользователя или сегмента.
    Лучшее распределение нагрузки. Даже активные клиенты не перегружают один узел.
    Масштабируемость по ширине. Легче равномерно расти за счёт увеличения числа нод.

[Схема 9: Anti-Affinity-шардирование]

Пример: В Salesforce логи активности клиента распределяются по разным шардам, чтобы избежать перегрузки одного узла.
Когда применять каждый подход

Выбор между affinity и anti-affinity нельзя считать универсальным — он зависит от требований системы:
Сценарий 	Предпочтительный подход
Высокочастотные JOIN-запросы 	Affinity
Низкий уровень доверия к одному узлу 	Anti-affinity
Работа с транзакциями 	Affinity
Доминирование одного клиента в нагрузке 	Anti-affinity
Высокие требования к отказоустойчивости 	Anti-affinity
Минимизация сетевых задержек 	Affinity

Практический вопрос: Как выбрать между affinity и anti-affinity для системы электронной коммерции?

    Ответ: Используйте affinity для связанных данных (например, пользователь и его заказы), чтобы ускорить транзакции и JOIN-ы. Для логов активности или аналитических данных применяйте anti-affinity, чтобы избежать горячих точек от активных пользователей. Гибридный подход: храните ключевые данные на одном шарде, а второстепенные распределяйте.

Некоторые современные системы (например, Spanner, CockroachDB, YugabyteDB) реализуют динамические стратегии размещения данных, где принципы affinity применяются гибко: в зависимости от текущей активности узел может "перетягивать" к себе связанные данные, чтобы уменьшить межшардовое взаимодействие.
Гибридный подход

На практике чаще всего используется гибридный подход: наиболее важные логические связи группируются внутри шарда, а второстепенные — распределяются по другим узлам. Например, можно гарантировать, что пользователь и его заказы всегда на одном шарде, но его активность в чатах или история входов может храниться отдельно.

Такой компромисс позволяет соблюсти баланс между производительностью, отказоустойчивостью и масштабируемостью.
Маршрутизация

Мы распределили данные по нескольким узлам на разных машинах, но остаётся важный вопрос: как клиенту определить, к какому узлу обратиться при выполнении запроса? Когда секции (шарды) перераспределяются — например, при добавлении или удалении узлов — меняется и их размещение. Поэтому нужен централизованный механизм, который будет отслеживать текущее состояние кластера и сможет сообщить клиенту куда обратиться чтобы получить нужные данные. В общем случае это называется Service Discovery, если эта тема интересна, разберем ее в следующей статье. А в частности — существует множество подходов к реализации service discovery в системах с шардированием и динамическим масштабированием. Вот несколько популярных решений и стратегий:

    Централизованный каталог (реестр) узлов: Системы вроде Consul, etcd или ZooKeeper хранят актуальную информацию о том, какие шарды где находятся. Клиенты или прокси могут запрашивать у них адрес нужного узла, отвечающего за конкретный диапазон данных.
    Прокси-роутер перед клиентами: Клиенты обращаются не напрямую к узлам, а к прокси-серверу, который уже знает всё о текущем состоянии шардинга и маршрутизирует запросы. Такой подход используется, например, в Vitess (MySQL) или Citus (PostgreSQL).
    Согласованное хэширование (Consistent Hashing): Клиент сам вычисляет, к какому узлу ему нужно обратиться, используя кольцо хэшей. Но при этом должен знать текущее состояние узлов (карту кольца), которая всё равно должна где-то поддерживаться — например, через тот же etcd.
    DNS-based Discovery (через обновляемые DNS-записи): адреса сервисов или узлов обновляются через DNS. Преимущество — простота, но минус в том, что обновления не мгновенные и не дают детального контроля.
    Kubernetes API для сервисов в k8s: В Kubernetes discovery встроен: клиенты узнают адреса подов и сервисов через Kube DNS или API Kubernetes. Также поддерживаются headless-сервисы для доступа напрямую к подам.

Практический вопрос: Как обеспечить надёжную маршрутизацию в Vitess?

    Ответ: Настройте Vitess Proxy для автоматического обновления карты шардов через ZooKeeper или etcd. Используйте метрики vtgate_query_latency для мониторинга задержек маршрутизации и настройте алерты на ошибки маршрутизации через Prometheus.

[Схема 10: Механизмы маршрутизации]
Кросс-шардовые запросы

При работе с распределёнными базами данных часто возникает необходимость выполнения запросов, которые требуют данных из нескольких шардов. Это приводит к нескольким техническим проблемам и увеличивает сложность обработки таких запросов. Рассмотрим основные методы решения этих проблем и подходы к оптимизации кросс-шардовых запросов.
Fan-out

Fan-out — это подход, при котором запрос рассылается на все шардовые узлы, и результаты агрегируются в едином месте. Такой метод полезен, когда необходимо выполнить однотипную операцию на всех шардах, например, когда нужно собрать информацию о пользователях с разных регионов или выполнить поиск по всем данным.

Проблемы:

    Производительность: чем больше шардов, тем дольше будет время отклика, так как запросы должны быть отправлены на каждый шард и потом собраны.
    Согласованность: могут возникнуть проблемы с синхронизацией данных между шардовыми узлами, особенно если на одном из шардов произошли изменения во время выполнения запроса.

Для оптимизации этого подхода часто используется кэширование промежуточных результатов, чтобы избежать повторной обработки одинаковых запросов. Преимущества:

    Простота реализации.
    Подходит для аналитических запросов, где важно собрать данные с нескольких источников.

[Схема 11: Fan-out запросы]

Практический вопрос: Как ускорить fan-out запросы в Citus?

    Ответ: Используйте citus.explain для анализа плана запроса. Если запрос затрагивает все шарды, денормализуйте данные или добавьте вторичные индексы. Кэшируйте результаты в Redis для повторяющихся запросов.

Federated Query Layer

Federated Query Layer — это промежуточный слой, который анализирует запрос, разбивает его на подзапросы и затем собирает итоговый результат. Этот подход позволяет выполнять кросс-шардовые запросы более эффективно, так как промежуточный слой может оптимизировать запросы, направлять их только на те шарды, которые содержат нужные данные, а затем агрегировать результаты.

Примером таких решений являются Citus или Vitess, которые могут анализировать запросы и автоматически распределять их между нужными шардовыми узлами, минимизируя нагрузку. Преимущества:

    Эффективное распределение запросов.
    Уменьшение нагрузки на каждый отдельный шард.
    Возможность реализации сложных аналитических запросов через единый интерфейс.

[Схема 12: Federated Query Layer]
Денормализация

Денормализация — это метод, при котором данные из разных таблиц или шардов могут дублироваться для минимизации необходимости в кросс-шардовых JOIN-ах. Этот подход помогает ускорить запросы, так как вместо того, чтобы делать JOIN между шардовыми узлами, все необходимые данные уже присутствуют в одном месте.

Такой подход используется в распределённых системах, где производительность важнее, чем точная нормализация данных. Например, можно создать отдельные таблицы с денормализованными данными, которые будут содержать всю информацию, необходимую для определённых типов запросов. Преимущества:

    Ускоряет выполнение запросов, уменьшив количество кросс-шардовых операций.
    Снижается нагрузка на сеть и увеличивается скорость обработки данных.

Недостатки:

    Увеличение объёма данных.
    Требуется дополнительная логика для синхронизации дублированных данных при изменениях.

[Схема 13: Денормализация данных]

Практический вопрос: Как синхронизировать денормализованные данные?

    Ответ: Используйте триггеры или materialized views в PostgreSQL для автоматического обновления денормализованных таблиц. В Vitess настройте VReplication для синхронизации данных между шардами.

Кросс-шардовые транзакции

Кросс-шардовые транзакции — это сложные транзакции, которые охватывают несколько шардов. Реализация таких транзакций требует особого подхода для обеспечения целостности и согласованности данных. Наиболее распространённым решением является использование двухфазного коммита (2PC).

2PC (Two-Phase Commit) — это протокол, который обеспечивает атомарность транзакций. Он состоит из двух фаз: в первой фазе все участники транзакции (шарды) подтверждают готовность выполнить операцию, а во второй — они либо подтверждают её выполнение, либо откатывают изменения.

Однако использование 2PC может значительно замедлить выполнение транзакций из-за необходимости ожидания подтверждений от всех шардов. Кроме того, в случае сбоя одного из участников система может попасть в состояние неопределённости, что потребует дополнительных механизмов для восстановления.

Другим подходом является использование eventual consistency, когда система соглашается на временное отсутствие полной согласованности, но гарантирует, что данные в конце концов станут согласованными. Преимущества:

    Обеспечивает согласованность и атомарность транзакций.
    Подходит для критичных операций, требующих высокой надёжности.

Недостатки:

    Высокие затраты на производительность и задержки.
    Потенциальные проблемы с восстановлением в случае сбоев.

[Схема 14: Двухфазный коммит]
Основные проблемы шардирования и их решения

Шардирование решает проблему масштабирования, но вводит эксплуатационные вызовы, такие как горячие точки, сложность кросс-шардовых запросов и необходимость ребалансировки. Ниже приведены ключевые проблемы, их последствия и методы решения.
Проблема 	Последствия 	Методы решения
Горячие точки (hot spots) 	Перегрузка отдельных шардов, задержки, снижение производительности 	- Согласованное хэширование
- Динамическая ребалансировка
- Мониторинг нагрузки через Prometheus/Grafana
Кросс-шардовые запросы 	Увеличение задержек, сложность обеспечения согласованности 	- Денормализация данных
- Federated Query Layer (Citus, Vitess)
- Кэширование результатов
Добавление нового шарда 	Потенциальный простой, необходимость перераспределения данных 	- Online resharding с dual writes
- Middleware (Vitess, Citus)
- Постепенная миграция данных
Несогласованность транзакций 	Ошибки в кросс-шардовых транзакциях, нарушение ACID 	- Двухфазный коммит (2PC)
- Eventual consistency для некритичных операций
- Логирование транзакций
Сложность маршрутизации 	Неверная маршрутизация запросов, увеличение задержек 	- Прокси-роутеры (ProxySQL, Vitess)
- Централизованный каталог (Consul, ZooKeeper)
- Согласованное хэширование
Неравномерная нагрузка 	Перегрузка части шардов, недозагрузка других 	- Автоматическая ребалансировка
- Асимметричное шардирование
- Мониторинг метрик (queries_per_shard)

[Схема 15: Проблемы шардирования и их решения]
Горячие точки

Горячие точки возникают, когда один шард получает непропорционально много запросов, например, при диапазонном шардировании, когда новые записи попадают в один диапазон.

Решения:

    Используйте согласованное хэширование для равномерного распределения.
    Мониторьте метрики нагрузки (queries_per_shard в Citus, cpu_usage_per_shard в Vitess) и настройте алерты через Prometheus.
    Перераспределяйте данные с помощью rebalance_table_shards (Citus) или Reshard (Vitess).

Пример: В социальной сети посты популярного блогера могут перегружать один шард. Решение — шардировать по хэшу post_id вместо user_id.
Кросс-шардовые запросы

Запросы, затрагивающие несколько шардов, увеличивают задержки и сложность из-за необходимости координации.

Решения:

    Денормализация: Дублируйте данные для минимизации JOIN-ов. Например, храните имя пользователя в таблице заказов.
    Federated Query Layer: Используйте Citus или Vitess для оптимизации запросов.
    Кэширование: Кэшируйте результаты в Redis или Memcached.

Добавление нового шарда

Решардирование может вызвать простои или ошибки, если данные переносятся неправильно.

Решения:

    Online resharding: Используйте dual writes для записи в старый и новый шард одновременно.
    Middleware: Citus и Vitess автоматизируют ребалансировку.
    Ручное решардирование: Подходит для небольших систем, но требует остановки записи.

Несогласованность транзакций

Кросс-шардовые транзакции могут нарушать ACID из-за задержек или сбоев.

Решения:

    Используйте 2PC для атомарности.
    Применяйте eventual consistency для некритичных операций.
    Используйте affinity-шардирование для минимизации кросс-шардовых транзакций.

Мониторинг и эксплуатация

Эффективная эксплуатация шардированной системы требует мониторинга ключевых метрик и настройки алертов.

    Метрики:
        Размер шардов (citus_shard_size в Citus).
        Количество запросов на шард (queries_per_shard).
        Время выполнения кросс-шардовых запросов.
        Прогресс ребалансировки (resharding_progress в Vitess).
    Инструменты:
        Prometheus + Grafana: Визуализация метрик шардов.
        Vitess Dashboard: Мониторинг состояния шардов и маршрутизации.
    Практический вопрос: Как настроить мониторинг горячих точек?
        Ответ: Используйте Prometheus для сбора метрик cpu_usage_per_shard и настройте алерты на превышение порога. Анализируйте запросы через EXPLAIN и перераспределяйте данные при необходимости.

[Схема 16: Мониторинг шардированной системы]
Компромиссы шардирования

Шардирование требует баланса между производительностью, сложностью и надёжностью:

    Простота vs. масштабируемость: Хэширование упрощает маршрутизацию, но усложняет аналитические запросы.
    Производительность vs. согласованность: Денормализация ускоряет запросы, но увеличивает объём данных и сложность синхронизации.
    Автоматизация vs. контроль: Middleware (Vitess, Citus) упрощает решардирование, но требует изучения и настройки.

Для социальной сети, где важна скорость, подойдёт хэширование и денормализация. Для финансовых систем, где критична согласованность, лучше использовать affinity-шардирование и 2PC.

[Схема 17: Сравнительная таблица компромиссов]
Шардирование и репликация

Секционирование часто используется вместе с репликацией, благодаря чему каждая секция данных имеет несколько копий, размещённых на разных узлах. Это означает, что хотя конкретная запись принадлежит только одной секции, она может физически храниться на нескольких узлах для повышения отказоустойчивости. Один узел может содержать несколько секций. В случае репликации по модели «ведущий — ведомый» структура распределения будет следующей: для каждой секции выбирается ведущий узел, который принимает запись, а остальные — ведомые, синхронизирующиеся с ним. При этом один и тот же узел может одновременно быть ведущим для одних секций и ведомым для других.

[Схема 18: Шардирование с репликацией]

Практический вопрос: Как комбинировать шардирование и репликацию для высокой доступности?

    Ответ: Настройте репликацию для каждого шарда (например, один мастер и несколько слейвов). Используйте прокси (Vitess Proxy или ProxySQL) для маршрутизации чтения к слейвам и записи к мастерам. Мониторьте задержки репликации через метрики (Seconds_Behind_Master в MySQL) и настройте автоматический failover с помощью Consul или ZooKeeper.

Вывод
Масштабирование реляционных баз данных — это важная задача для обеспечения их высокой производительности и доступности в условиях растущих объемов данных и запросов. Хотя реляционные БД традиционно славятся своей консистентностью и надежностью, с ростом требований к скорости обработки и масштабируемости, многие из них начинают сталкиваться с вызовами. Для решения этих проблем применяются различные подходы, такие как вертикальное и горизонтальное масштабирование, репликация и использование распределенных систем. Каждый из этих методов позволяет эффективно справляться с увеличивающейся нагрузкой, сохраняя при этом принципы работы реляционных БД. В будущем для достижения оптимальной производительности и гибкости, возможно, потребуется комбинированный подход, который объединяет традиционные реляционные технологии с новыми решениями, такими как NoSQL и распределенные системы, чтобы обеспечить быстрое и эффективное управление данными на всех уровнях. В процессе написания данной статьи собралось большое количество интересного материала который нам будет приятно осветить в дальнейших публикациях.
