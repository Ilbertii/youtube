## Введение

Информационные системы в современном мире сталкиваются с **необходимостью обработки растущих объемов данных**, что предъявляет высокие требования к производительности и отказоустойчивости баз данных. Реляционные СУБД остаются популярными благодаря своей надежности, строгой схеме данных и поддержке транзакций ACID. Однако при увеличении нагрузки традиционные подходы к проектированию реляционных баз данных могут стать узким местом, приводя к замедлению работы приложений и ухудшению пользовательского опыта.

Масштабирование реляционных баз данных — это комплекс методов, направленных на распределение нагрузки и обеспечение высокой доступности системы. Оно может быть **вертикальным** (увеличение ресурсов сервера) или **горизонтальным** (распределение данных между несколькими узлами). Каждый из этих подходов имеет свои преимущества и ограничения, а их выбор зависит от специфики приложения, бюджета и требований к отказоустойчивости.

В данной статье рассматриваются самые важные стратегии масштабирования реляционных баз данных, включая шардирование и репликацию. Также обсуждаются ключевые проблемы, такие как обеспечение согласованности данных, управление распределенными транзакциями и минимизация задержек в географически распределенных системах.

## Как можно масштабировать реляционные базы данных?

Реляционные базы данных — основа многих приложений, но с ростом нагрузки их производительность может снижаться. Чтобы решить эту проблему, применяют два основных подхода: **вертикальное** и **горизонтальное** масштабирование.

### Вертикальное масштабирование

**Вертикальное масштабирование** остается самым простым и предсказуемым способом увеличения производительности реляционных СУБД. Этот подход подразумевает улучшение характеристик сервера: добавление процессорных ядер, увеличение оперативной памяти, использование более быстрых дисковых подсистем. Он не требует изменений в архитектуре приложения и сохраняет все преимущества реляционной модели. Главное достоинство вертикального масштабирования — его концептуальная простота. Администратору не нужно перестраивать логику работы приложения или беспокоиться о согласованности распределенных данных. Все транзакции продолжают выполняться на одном узле, гарантируя соблюдение принципов ACID. Это особенно важно для финансовых систем и приложений, где критична точность данных. Технически масштабирование сводится к переносу БД на более мощный сервер или изменению конфигурации облачного инстанса. Например, переход с 4-ядерной виртуальной машины на 16-ядерную может дать немедленный прирост производительности без необходимости изменять SQL-запросы или схему данных.

Однако у этого метода есть принципиальные ограничения. Физические пределы серверного оборудования создают естественный "потолок" для масштабирования. Современные серверы верхнего уровня могут иметь до нескольких терабайт RAM и сотни процессорных ядер, но их стоимость становится непропорционально высокой. Экономика облачных решений усугубляет эту проблему. Цены на мощные инстансы растут нелинейно - переход с 8-ядерной конфигурации PostgreSQL на 64-ядерную может увеличить стоимость в 10-15 раз при реальном приросте производительности всего в 4-6 раз. Кроме того, остается проблема единой точки отказа - выход сервера из строя парализует всю систему.

Вертикальное масштабирование хорошо подходит для систем с предсказуемым и умеренным ростом нагрузки. Например, специализированные отраслевые решения часто десятилетиями работают на вертикально масштабируемых конфигурациях. Другой типичный случай - стартапы на этапе активного роста. Многие успешные компании сознательно откладывали переход на распределенные архитектуры, последовательно увеличивая мощность серверов. Это позволяло сосредоточиться на развитии продукта, а не на перестройке инфраструктуры.

Важно понимать, что вертикальное масштабирование - это временное решение. Как показывает опыт высоконагруженных систем, рано или поздно наступает момент, когда дальнейшее наращивание мощности сервера становится невозможным или экономически нецелесообразным. Однако грамотное использование этого метода позволяет отложить сложный переход к горизонтальному масштабированию до того момента, когда компания будет готова к таким изменениям технически и организационно.

### Горизонтальное масштабирование

**Горизонтальное масштабирование** предполагает добавление новых серверов или узлов в распределённую систему, что позволяет распределить нагрузку между несколькими экземплярами базы данных. Горизонтальное масштабирование может быть реализовано через **шардинг** и **репликацию**.

## Репликация

**Репликация** — это процесс создания копий базы данных на разных серверах, которые могут использоваться для обработки запросов на чтение. Репликацию можно классифицировать по следующим характеристикам:
- **По архитектуре репликации** между собой выделяют: репликация с одним ведомым узлом (Master-slave replication), репликация с несколькими ведомыми узлами (Multi-master replication), репликация без ведомых узлов (Masterless replication).
- **По принципу синхронизации** на остальные узлы выделяют: асинхронную, синхронную и каскадную репликацию.
- **По уровню передачи данных** на остальные узлы выделяют: физическую и логическую репликацию.

**..тут можно ещё что-то добавить, чтобы текст читался более лаконично...**

### Master-Slave репликация

Один из наиболее распространённых подходов — **Master-Slave репликация**.

[Схема 1: схема работы Master-slave репликации]

Эта модель предполагает строгое разделение ролей между узлами: один сервер (master) принимает все операции записи и какую-то часть операций на чтение, а один или несколько подчинённых серверов (slave) синхронизируют данные и обслуживают запросы только на чтение.

Давайте представим, как на самом деле работает эта система. Когда вы отправляете запрос на изменение данных - будь то добавление новой записи (INSERT), обновление существующей (UPDATE) или удаление (DELETE) - всё начинается с **мастер-узла**. Это главный распорядитель, который первым принимает все изменения. Но самое интересное начинается потом, когда эти изменения нужно разослать подчинённым узлам. Здесь система предлагает два принципиально разных подхода, каждый со своей особенностью. Первый - это **физическая репликация**, где данные передаются в своём "сыром", нетронутом виде, как есть

[Схема 1: формат данных, который передаётся из узела в узел]

Представьте, что мастер просто фотографирует свои бинарные логи - те самые WAL в PostgreSQL или binlog в MySQL - и рассылает эти снимки слейвам. Красота этого метода в его простоте и точности: реплики получают данные в точно таком же виде, в каком они существуют на мастере. Но за эту точность приходится платить - все узлы должны быть похожи как близнецы, работать на совместимом железе и одинаковых версиях ПО.

А теперь второй подход - **логическая репликация** - это уже более интеллектуальный способ общения между узлами

[Схема 2: формат данных, который передаётся из узела в узел]

Здесь мастер не просто копирует данные, а фактически пересказывает, что именно произошло: "Я только что добавил новую запись в таблицу пользователей", или "Я обновил цену в товаре с ID 42". Это как разница между отправкой фотографии документа и его кратким изложением по телефону. Такой подход куда более гибкий - он позволяет реплицировать только отдельные таблицы, работать с разными версиями СУБД и даже немного корректировать данные по пути.

И в этом танце данных есть ещё один важный ритм - **принцип синхронизации**, который будет рассмотрен ниже.

#### Принципу синхронизации данных между узлами

В системах репликации данных применяются два принципиально разных подхода, различающихся по своим гарантиям и характеристикам производительности. Рассмотрим их ключевые особенности.

**Синхронная репликация** обеспечивает максимальную надежность за счет строгого порядка операций. При получении запроса на запись мастер-узел сначала рассылает данные всем репликам и ожидает явного подтверждения от каждой из них. Только после получения подтверждений от всех реплик (или заданного кворума) операция считается успешной и клиент получает ответ. Такой подход гарантирует сохранность данных и их согласованность на всех узлах, но имеет существенные ограничения. Производительность системы определяется самым медленным узлом, а географическая распределенность значительно увеличивает задержки. Главный недостаток - отказ даже одной реплики приводит к невозможности подтверждения операций, что может парализовать всю систему.

[Схема 4: последовательность при синхронной репликации]

**Асинхронная репликация** предлагает противоположный компромисс. Мастер-узел подтверждает запись клиенту сразу после локального выполнения, не дожидаясь обработки репликами. Изменения передаются подчиненным узлам в фоновом режиме по мере возможности. Это обеспечивает высокую производительность и устойчивость к временным сбоям реплик, поскольку система продолжает работать даже при их недоступности. Однако такой подход несет риски потери данных - если мастер выйдет из строя до завершения синхронизации, последние изменения могут быть утрачены. Кроме того, реплики могут временно содержать неактуальные данные.

[Схема 3: последовательность действий при асинхронной репликации]

Основное различие между подходами заключается в ключевом компромиссе распределенных систем - между надежностью и производительностью. Синхронная репликация обеспечивает максимальную сохранность данных, но ценой снижения скорости работы и доступности системы. Асинхронная репликация, напротив, предлагает высокую производительность и отказоустойчивость, однако допускает возможность потери данных и временной несогласованности - эти аспекты мы подробно рассмотрим далее.

| Проблема |	Последствия	| Методы решения |
|----------|--------------|----------------|
| Временная рассогласованность | Пользователи видят устаревшие данные |	- Временные маркеры "обновляется" - Чтение после записи (read-after-write) для критичных данных
| Риск потери данных | Последние изменения пропадают при сбое мастера |	- Полусинхронная репликация для критичных операций - WAL-логи с подтвержденной записью
| Конфликты обновлений | Конкурирующие изменения на разных узлах | - Векторные часы для определения порядка - Алгоритмы разрешения конфликтов (CRDT)
| Накопление отставания |	Реплики не успевают за мастером при нагрузке | - Автомасштабирование реплик - Ограничение скорости записи

##### Потеря данных при сбое основной ноды

Асинхронная репликация подразумевает, что данные сначала записываются на мастер ноду, а затем (с задержкой, другими словами создаётся окно неконсистентности данных) отправляются на реплики. Если основная нода выходит из строя до того, как изменения попали на реплики, такие данные теряются.

**Полу-синхронная репликация** (Semi-synchronous replication) - включает промежуточный этап, при котором транзакция считается завершённой только после того, как хотя бы одна реплика подтвердила получение данных. Это снижает вероятность потери данных сохраняя приемлемую производительность.

**Журналирование на клиентской стороне** - клиенты могут логировать транзакции до подтверждения их основным узлом и повторно отправлять при сбое. Однако этот подход усложняет архитектуру.

**Введение промежуточного кэширующего слоя (например, WAL Proxy)** - Данные сначала пишутся в устойчивый лог, который одновременно ретранслируется в мастера и реплики.

##### Задержка репликации (replication lag)

Из-за отсутствия синхронизации в реальном времени, данные на репликах могут отставать от мастера на значительное время, особенно под высокой нагрузкой.

**Мониторинг и алерты по lag-метрикам** - использование метрик вроде Seconds_Behind_Master (MySQL) позволяет отслеживать задержку. В случае повышения порогов можно временно исключить реплику из пула чтения.

**Разделение нагрузки между мастером и репликами** - Минимизация чтений/записей, чтобы уменьшить отставание.

**Использование более эффективных протоколов репликации** - например, физическая репликация быстрее логической, особенно при большом количестве DML-операций.

**Аппаратные улучшения** - быстрые диски (NVMe), оптимизация сетевого стека, увеличесние буферов и размер пакетов в настройках репликации.

##### Несогласованность данных

Из-за задержки данные на репликах не гарантируют точного соответствия мастеру. Это особенно критично для сценариев, где читается недавно записанная информация (Read-Your-Writes Consistency).
https://arpitbhayani.me/blogs/read-your-write-consistency/
**Согласованное маршрутизирование запросов** - использование middleware или proxy, которые маршрутизируют клиентские запросы так, чтобы клиенты после записи всегда читали с мастера.

**Версионирование данных** - система может отслеживать версии объектов и сравнивать их между узлами, обеспечивая согласованность при необходимости.

**Использование CRDT** (???) - для специфических доменов можно применять структуры, допускающие асинхронную репликацию с разрешением конфликтов без потери данных.

##### Failover и Split-Brain

В случае выхода мастера из строя и автоматического переключения на реплику може произойти ситуация split-brain - одновременная запись в два несовместимых источника истины.

**Использование механизмов quorum'a** - репликация и выбор мастера осуществляется на основе большинства (например, через Raft или Paxos), исключая возможность split-brain

**Fencing токены (например, Zookeeper fencing)** - каждому лидеру выдаётся уникальный токен, предотвращающий "двойное лидерство"

**Использование consensus-сервисов** - etcd, consul, zookeeper обеспечивает отказоустойчивый контроль за лидерством и конфигурацией.

**Выбор стратегий автоматического или ручного failover** - в критичных системах предпочтительнее использовать ручной failover с предварительным анализом состояния реплик.

На практике часто используют **гибридные решения**. Например, в PostgreSQL можно настроить синхронную репликацию только для критически важных транзакций, а всё остальное пускать асинхронно.

[Схема 5: пример гибридной настройки]

Или сделать синхронной только одну из нескольких реплик — это даёт баланс между надёжностью и производительностью.

Главный выбор здесь — между скоростью и надёжностью. Для социальной сети, где можно на секунду потерять последний лайк, подойдёт асинхронный вариант. А для банковских операций или медицинских систем, где каждая запись должна быть сохранена наверняка, без синхронной репликации не обойтись

[Схема 6: сравнительная таблица подходов]

### Multi-Master репликация

Представьте базу данных, где не один, а сразу несколько серверов могут принимать запросы на запись. Это не классическая master-slave схема, к которой многие привыкли, а более современный и гибкий подход — multi-master репликация. Но в чём принципиальная разница между этими подходами и когда стоит выбирать каждый из них? Почему это вообще понадобилось?

Классическая master-slave схема долгое время была стандартом де-факто. Один главный сервер принимает все записи, а его подчинённые лишь читают данные. Просто, надёжно, но... Когда приложение растёт, этот единственный мастер превращается в узкое горлышко. Представьте стадион с сотней входов, но только одной кассой — очереди неизбежны. А если она еще окажется недоступной...

Именно здесь multi-master репликация предлагает элегантное решение. Теперь касс несколько, и посетители могут выбирать любую. Для глобальных сервисов это особенно ценно — пользователи в разных частях света пишут в ближайший к ним узел, не испытывая задержек из-за расстояний.

Как это работает в реальности?

Допустим, у нас два мастера — в Москве и во Владивостоке. Пользователь из Приморья вносит изменения, которые сначала попадают на локальный узел. Фокус в том, что система не заставляет его ждать, пока данные доберутся до московского сервера. Вместо этого она говорит: "Хорошо, я приняла твои изменения здесь, а синхронизацией займусь потом".

Это "потом" — ключевой момент. Репликация происходит асинхронно, что даёт потрясающую отзывчивость, но создаёт интересные ситуации. Например, пока изменение путешествует между узлами, два пользователя в разных городах могут видеть разное состояние одних и тех же данных. Но всё не так просто, как кажется.

Главная головоломка multi-master — конфликты. Что если два мастера одновременно изменят одну и ту же запись? Здесь в игру вступают различные стратегии:
- Можно просто взять последнее изменение по времени (Last Write Wins)
- Или попытаться объединить изменения (как это делает Git при мердже веток)
- А иногда приходится звать администратора, чтобы он вручную разобрался, чья правда важнее

Ещё один нюанс — производительность. Казалось бы, больше серверов — больше мощности. Но вся эта синхронизация между узлами съедает немало ресурсов. Порой проще нарастить один мощный сервер, чем городить кластер.

Это действительно нужно если ваше приложение:
- Распределено географически
- Испытывает высокую нагрузку на запись
- Может мириться с временными несоответствиями данных
- Должно работать без Интернета
- Поддерживаает совместное редактирование

Multi-master может стать отличным выбором. Но для банковских систем или там, где важна абсолютная согласованность, лучше поискать другие варианты.

##### Каскадная репликация

Это тип репликации, при котором слейв-сервер может быть реплицирован на другие слейв-сервера. В этой архитектуре изменения, сделанные на **мастер-сервере**, сначала транслируются на первый уровень реплик, а затем от них — к последующим репликам.

Преимущества каскадной репликации:
- **Оптимизация нагрузки**: Избегается перегрузка мастера и первого уровня реплик, так как реплики могут делегировать часть своих запросов другим репликам.
- **Масштабируемость**: Легко расширять систему за счёт добавления новых слейв-реплик на каждом уровне, что позволяет эффективно распределять нагрузку.
- **Уменьшение трафика на мастер-сервере**: Мастер-сервер загружен только первичной репликацией, а остальные реплики могут передавать данные каскадно, снижая нагрузку на основную базу данных.

Недостатки каскадной репликации:
- **Дополнительная задержка**: Из-за того, что данные передаются через несколько уровней реплик, возникает дополнительная задержка в репликации данных, что может привести к рассогласованию на разных уровнях.
- **Сложность в настройке и управлении**: Поддержка каскадной репликации требует более сложной настройки и мониторинга, так как нужно отслеживать состояние всех уровней реплик.

### Репликация без ведущего узла




В следующей части будет рассмотрена тема шардирования
