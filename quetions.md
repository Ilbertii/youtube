# Как обеспечивается надежность и консистентность в распределенных базах данных?

При работе с распределенными базами данных возникают вопросы, связанные с консистентностью данных, отказоустойчивостью и восстановлением узлов после сбоев. Разберем, как эти проблемы решаются на практике.

## Как избежать устаревших данных при чтении с реплик?

Когда данные записываются в мастер-узел, на репликах они появляются с задержкой. Это может привести к ситуации, когда клиент читает устаревшие данные. Есть несколько способов минимизировать такие риски:

1. **Настройка согласованных чтений** – механизмы вроде `read-your-writes` (гарантия, что клиент увидит свои изменения) или `linearizable` (полная согласованность, как если бы все операции выполнялись строго последовательно) позволяют читать только актуальные данные.
2. **Использование quorum-стратегии** – система требует ответа от большинства реплик перед возвратом данных, что снижает вероятность устаревших данных. В Cassandra можно задать `QUORUM`-чтение, а в MongoDB использовать `majority`.
3. **Sticky-сессии** – запросы от одного клиента всегда направляются на одну и ту же реплику, снижая вероятность несогласованности данных.
4. **Механизмы Read Repair и Anti-Entropy** – в некоторых БД (например, Cassandra) устаревшие данные могут исправляться в фоновом режиме при чтении или с помощью регулярной синхронизации между узлами.

## Что происходит при отказе мастера?

Когда мастер-узел выходит из строя, необходимо выбрать новый мастер и обновить маршрутизацию трафика. Это делается в несколько этапов:

1. **Обнаружение отказа** – кластеры используют heartbeat-механизмы (например, Keepalived, Patroni), мониторинг (Prometheus, Orchestrator, Sentinel для Redis) или распределенные алгоритмы (Raft, Paxos) для определения сбоя.
2. **Выбор нового мастера**:
   - В PostgreSQL с Patroni выбор происходит через Raft.
   - В MySQL используется Orchestrator, который выбирает новую реплику на основе `GTID`.
   - В Redis Sentinel голосует за новый мастер.
   - В Kafka Zookeeper координирует распределение лидера разделов.
3. **Переключение клиентов** – обновляются DNS-записи, виртуальные IP (VIP) в балансировщиках (например, HAProxy, PgBouncer) или сервисах обнаружения (Consul, Zookeeper).
4. **Синхронизация данных** – бывшие реплики догоняют новый мастер, а отставшие узлы перезапускаются с актуального snapshot’а или получают недостающие WAL/GTID-логи.

## Как вернуть в балансировку упавшую или новую реплику?

После восстановления узла его нужно снова подключить к кластеру, но важно убедиться, что он содержит актуальные данные. Это достигается следующим образом:

1. **Проверка целостности**:
   - В PostgreSQL можно использовать `pg_checksums` для проверки данных.
   - В MySQL – `CHECK TABLE` или `mysqlcheck`.
   - В файловых системах – `fsck` для проверки целостности.
2. **Синхронизация с мастером**:
   - Если реплика сильно отстала, проще создать новую копию с мастера (`pg_basebackup`, `XtraBackup` для MySQL, `rdb`/`aof` файлы в Redis).
   - Если отставание небольшое, можно догнать изменения через WAL-логи (PostgreSQL) или GTID (MySQL).
3. **Постепенное добавление в балансировщик**:
   - Сначала тестируется производительность узла (`pg_stat_replication`, `SHOW SLAVE STATUS` в MySQL).
   - После успешного тестирования узел включается в пул запросов.
4. **Мониторинг** – Prometheus, Grafana, ELK помогают следить за метриками и поведением узла.

## Где хранятся служебные данные о кластере?

Для управления кластером и хранения его состояния используются различные механизмы:

- **Распределенные key-value хранилища**:
  - **Etcd** (используется в Kubernetes, CoreDNS) – управляет конфигурацией и статусом узлов.
  - **Zookeeper** (Kafka, HBase, Hadoop) – координирует распределенные процессы.
  - **Consul** (HashiCorp) – хранит информацию о сервисах и их состоянии.
- **Встроенные механизмы БД**:
  - PostgreSQL хранит метаданные в `pg_stat_activity`, `pg_replication_slots`.
  - MySQL предоставляет `performance_schema`, `SHOW SLAVE STATUS` для мониторинга репликации.
- **Объектные хранилища для резервных копий**:
  - Amazon S3, Google Cloud Storage, Azure Blob – долговременное хранение WAL-логов и снапшотов.
  - Ceph/Rook – распределенное хранилище, используемое для хранения данных в крупных кластерах.
- **Системы логирования и мониторинга**:
  - **Loki, ELK (Elasticsearch + Logstash + Kibana)** – централизованное логирование.
  - **Prometheus + Grafana** – мониторинг задержек, ошибок и нагрузки на узлы.

## Заключение

Обеспечение консистентности и надежности распределенных систем – сложная, но решаемая задача. Различные механизмы репликации, автоматическое восстановление после отказов и продуманная архитектура помогают создать устойчивые к сбоям системы. Важно грамотно выбирать стратегии репликации, следить за здоровьем узлов и использовать современные инструменты мониторинга.

Использование технологий вроде Raft, GTID, WAL, quorum-стратегий и сервисов типа Etcd, Consul, Zookeeper позволяет построить отказоустойчивый кластер, минимизируя риск потери данных и простоев.


