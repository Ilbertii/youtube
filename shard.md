Как определяется, в каком шарде находится нужные данные?

Существует несколько подходов:

    Клиент знает алгоритм маппинга и сам вычисляет нужный шард.

    Прокси-слой (router) принимает запросы и перенаправляет их на нужный шард.

    Центральное хранилище маппинга (например, etcd, ZooKeeper) — для более гибкой, но сложной маршрутизации.

Важно: маппинг ключей к шардам должен быть согласованным и отказоустойчивым. При его изменении необходимо минимизировать перетасовку данных (используется consistent hashing, range rebalance и т.д.).
Как масштабировать систему по мере роста данных?

Добавление новых шардов требует перераспределения данных:

    С хеш-алгоритмом — применяется consistent hashing для минимизации миграции.

    С диапазонным шардингом — создаются новые диапазоны и перераспределяются ключи.

Масштабирование должно быть автоматизировано, либо хотя бы иметь инструменты миграции с отслеживанием прогресса, проверками целостности и откатами.
Что происходит при отказе одного шарда?

Если каждый шард — это один физический узел, то его сбой означает недоступность части данных. Это принципиальное отличие от репликации: в шардинге не все данные дублируются.

Чтобы повысить отказоустойчивость:

    Каждый шард может быть кластером с репликами (но это уже не про сам шардинг, а про устойчивость внутри него).

    Прокси-слой или клиенты должны уметь корректно обрабатывать временную недоступность части шардов.

    Важно мониторить каждый шард и держать алерты по latency, error rate, CPU/Disk usage.
Где и как хранятся метаданные о шардах?

В крупной системе необходимо централизованное хранилище топологии:

    etcd, ZooKeeper, Consul — хранят конфигурацию шардов, их состояние и маршруты.

    При отказе или обновлении шарда — эта информация обновляется автоматически или вручную через инструменты orchestration-а.

    Важно обеспечить отказоустойчивость самого хранилища метаданных, иначе весь кластер "ослепнет".
Как мониторить и поддерживать здоровье шардированной системы?

    Каждый шард должен быть отдельной единицей мониторинга: метрики запросов, задержек, ошибок.

    Нужен централизованный трейсинг и лог-корреляция, чтобы отслеживать кросс-шардовые запросы.

    Используется shard-aware алертинг — например, если один шард начинает резко отставать по скорости отклика.
Как избежать конфликтов и "перекрёстных" операций между шардами?

По умолчанию, в шардинге каждый ключ принадлежит только одному шарду — и это упрощает логику. Но в сложных сценариях (например, перевод денег между пользователями из разных шардов):

    Нельзя использовать обычные транзакции — нужны распределённые (2PC, Saga).

    Часто проще изменить модель данных или бизнес-логику, чем реализовать кросс-шардовые транзакции.
росс-шардовые транзакции — сложно, дорого и опасно

Если операция затрагивает несколько шардов одновременно, то обычная транзакция не работает. Есть несколько подходов:

    2-Phase Commit (2PC) — координация между шардами:

        Все участники подготавливают данные (prepare),

        После подтверждений — происходит commit.

        ❗ Минусы: сложно реализуется, медленно, возможны блокировки и "подвисания".

    Сага (Saga pattern) — разбиение на набор локальных транзакций с компенсацией:

        Каждое действие атомарно,

        В случае ошибки вызываются "откатывающие" действия.

        ✅ Более гибкий и подходящий для микросервисов и событийных архитектур.

    Изменение модели данных:

        Сделать так, чтобы связанные данные всегда попадали в один шард (например, все заказы пользователя → в один шард).

        Это позволяет полностью избежать кросс-шардовых операций.
